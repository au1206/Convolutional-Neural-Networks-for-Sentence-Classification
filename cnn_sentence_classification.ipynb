{
  "cells": [
    {
      "metadata": {
        "_uuid": "33f5679b5e95039a32bd91c4d9fe6e55c3e17462"
      },
      "cell_type": "markdown",
      "source": "**INTRODUCTION**\n\nHi all, this kernel is an intorduction to text classification using deep leanring. It took some time for the deep learning approaches to make a mark on textual data but since then the impact of deep learning on NLP has had a vertical graph. \n\nIn this kernel we will get our hands dirty with a well in demand problem of text/document classification, around 2014 yoon kim et al. started to experiment with the relevance of CNN in the field of NLP and since then there has been no looking back. In the paper \"[Convolutional Neural Networks for Sentence Classification](http://arxiv.org/pdf/1408.5882.pdf)\" yoon kim et al. experiments with multiple CNN models (single channel, multiple channel) on top of word embeddings for text classification.\n\nFor the sake of simplicity we will start off with a single channel model with pretrasined Glove embeddings. The data set used is the famous [20_newsgroup dataset](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)(original dataset link).\n\nIn this kernel we will first learn about the processing of dataset, followed by a keras implementation of text classification using the preexisting Glove embeddings. \n"
    },
    {
      "metadata": {
        "_uuid": "a80121ade4ecf6c4caa264a5fd5cd5981778119a"
      },
      "cell_type": "markdown",
      "source": "**THE APPROACH**\n\nThe idea presented follows a flow like : \n<a href=\"https://imgur.com/xLrP6IM\"><img src=\"https://i.imgur.com/xLrP6IM.png\" title=\"source: imgur.com\" style=\"width:400px;height:600px;\"/></a>\n\n\nWe basically add different convolution layers of filter sizes [3, 4, 5], this somewhat emulates different skip-gram models where different filter sizes essentially means the number of words the filter is being applied to. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0750388a2837b4425e5f1dd11da60a8f9b30c7b4"
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport numpy as np\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\nfrom keras.layers import MaxPool1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "daf453aefbb50f26e525042203cfcbf4b3976c57"
      },
      "cell_type": "code",
      "source": "# just to make sure the dataset is added properly \n!ls '../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35297be0266ec89dc1786312025a26458be584d6"
      },
      "cell_type": "code",
      "source": "# the dataset path\nTEXT_DATA_DIR = r'../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n#the path for Glove embeddings\nGLOVE_DIR = r'../input/glove6b/'\n# make the max word length to be constant\nMAX_WORDS = 10000\nMAX_SEQUENCE_LENGTH = 1000\n# the percentage of train test split to be applied\nVALIDATION_SPLIT = 0.20\n# the dimension of vectors to be used\nEMBEDDING_DIM = 100\n# filter sizes of the different conv layers \nfilter_sizes = [3,4,5]\nnum_filters = 512\nembedding_dim = 100\n# dropout probability\ndrop = 0.5\nbatch_size = 30\nepochs = 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa68305c64bc844cd86e776b3cb37e3661f6f652"
      },
      "cell_type": "markdown",
      "source": "**DATASET STRUCTURE**\n\nThe dataset is present in a hierarchical structure, i.e. all files of a given class are located in their respective folders and each datapoint has its own '.txt' file.\n\n* First we go through the entire dataset to build our text list and label list. \n* Followed by this we tokenize the entire data using Tokenizer, which is a part of keras.preprocessing.text.\n* We then add padding to the sequences to make them of a uniform length."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f9972b8b95a38df4e08227b5a638bd675d7c945"
      },
      "cell_type": "code",
      "source": "## preparing dataset\n\n\ntexts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                if sys.version_info < (3,):\n                    f = open(fpath)\n                else:\n                    f = open(fpath, encoding='latin-1')\n                t = f.read()\n                i = t.find('\\n\\n')  # skip header\n                if 0 < i:\n                    t = t[i:]\n                texts.append(t)\n                f.close()\n                labels.append(label_id)\nprint(labels_index)\n\nprint('Found %s texts.' % len(texts))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3058f0c6703374a384d8720712cb2151e44e8ca"
      },
      "cell_type": "code",
      "source": "tokenizer  = Tokenizer(num_words = MAX_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences =  tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint(\"unique words : {}\".format(len(word_index)))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\nprint(labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc1c709458e9f4eb338a40c40c85dedba29c6fe8"
      },
      "cell_type": "code",
      "source": "# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ba8039ec130a51f64bad77c718a7f2e91e13d19"
      },
      "cell_type": "markdown",
      "source": "Since we have our train-validation split ready, our next step is to create an embedding matrix from the precomputed Glove embeddings.\nFor convenience we are freezing the embedding layer i.e we will not be fine tuning the word embeddings. Feel free to test it out for better accuracy on very specific examples. From what can be seen, the Glove embeddings are universal features and tend to perform great in general."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0620c11d2dab62329f250ecad40bcefbf57a7134"
      },
      "cell_type": "code",
      "source": "embeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07d064695cf65aaba497d6bb0dbd14dea220d533"
      },
      "cell_type": "code",
      "source": "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74abe6ec0048d25c6169081f7cd409359588aee0"
      },
      "cell_type": "code",
      "source": "from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e499119a397f180258ab0e2b8c5a6b47ef98fc7c"
      },
      "cell_type": "code",
      "source": "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding = embedding_layer(inputs)\n\nprint(embedding.shape)\nreshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\nprint(reshape.shape)\n\nconv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\nconv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\nconv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n\nmaxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n\nconcatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\nflatten = Flatten()(concatenated_tensor)\ndropout = Dropout(drop)(flatten)\noutput = Dense(units=20, activation='softmax')(dropout)\n\n# this creates a model that includes\nmodel = Model(inputs=inputs, outputs=output)\n\ncheckpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3f99fb84e45c5fdf63607020c346902c340a31a",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(\"Traning Model...\")\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(x_val, y_val))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6dd11fdc4f1d2898adb4103d7794a7d4a3919d4c"
      },
      "cell_type": "markdown",
      "source": "I hope this Kernel was helpful for you, any sort of feedback and comments are appreciated. Feel free to reach out in case something is unclear.\n\nUntil next time, Happy learning :) . . .. ..."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "4da2bc289e1d27d5225f68cb33352347e737c8a4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}